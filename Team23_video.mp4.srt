1
00:00:00,890 --> 00:00:01,270
Hello,

2
00:00:01,450 --> 00:00:02,390
this is team 23.

3
00:00:03,510 --> 00:00:05,290
This is our final project video,

4
00:00:06,030 --> 00:00:07,950
and this is what we will talk about here.

5
00:00:10,250 --> 00:00:13,790
This is an era where baseball heavily relies on data science,

6
00:00:13,990 --> 00:00:16,110
but most current analyses are

7
00:00:16,110 --> 00:00:18,890
primarily based on mathematical statistics.

8
00:00:19,630 --> 00:00:22,950
We believe these analyses overlook the impact of

9
00:00:22,950 --> 00:00:26,890
the game context and fail to uncover more abstract features,

10
00:00:27,310 --> 00:00:29,270
such as the synergy between the pitcher

11
00:00:29,270 --> 00:00:29,990
and catcher,

12
00:00:30,250 --> 00:00:33,730
or how a pitcher's strategy shifts depending on the count.

13
00:00:34,550 --> 00:00:35,390
To address this,

14
00:00:35,610 --> 00:00:37,110
we employ RNN models,

15
00:00:37,510 --> 00:00:40,670
comparing various data slicing methods and different

16
00:00:40,670 --> 00:00:42,190
model stacking approaches,

17
00:00:42,590 --> 00:00:46,370
aiming to detect the hidden insights that are not immediately

18
00:00:46,370 --> 00:00:47,990
apparent in the raw data.

19
00:00:49,140 --> 00:00:49,620
Okay,

20
00:00:49,680 --> 00:00:50,980
I'm going to talk about data collection.

21
00:00:51,200 --> 00:00:53,760
We easily use the package called PyBaseball,

22
00:00:54,040 --> 00:00:55,460
and when we read the documentation,

23
00:00:55,820 --> 00:00:56,280
we find

24
00:00:56,280 --> 00:00:58,460
Now that we can easily input the first name,

25
00:00:58,540 --> 00:00:58,920
last name,

26
00:00:59,080 --> 00:01:00,600
also the start date and end date,

27
00:01:00,920 --> 00:01:02,420
and then we can get the CSV,

28
00:01:02,800 --> 00:01:04,099
and this is the raw data structure.

29
00:01:06,040 --> 00:01:08,700
Now I'm going to introduce data pre-processing.

30
00:01:08,920 --> 00:01:11,880
We normalize the numeral feature such as

31
00:01:12,120 --> 00:01:12,900
release,

32
00:01:13,300 --> 00:01:13,580
speed,

33
00:01:13,680 --> 00:01:13,880
launch,

34
00:01:13,980 --> 00:01:14,160
speed,

35
00:01:14,340 --> 00:01:14,580
launch,

36
00:01:14,700 --> 00:01:14,940
angle,

37
00:01:15,100 --> 00:01:15,960
and hit distance.

38
00:01:17,980 --> 00:01:21,460
Then we use one heart to handle categorical feature

39
00:01:21,460 --> 00:01:22,680
like pitch type,

40
00:01:22,840 --> 00:01:23,180
stripes,

41
00:01:23,300 --> 00:01:23,540
balls,

42
00:01:23,660 --> 00:01:24,080
and zone.

43
00:01:25,310 --> 00:01:27,190
And then we add the inferencing feature.

44
00:01:27,190 --> 00:01:31,890
we add additional features based on the frequency of different piece types.

45
00:01:35,900 --> 00:01:36,340
Next up,

46
00:01:36,400 --> 00:01:38,060
I'm going to talk about our model design.

47
00:01:38,580 --> 00:01:40,620
Our first model is an RNN-based model.

48
00:01:41,120 --> 00:01:42,980
The masking layer is to help ensure that

49
00:01:42,980 --> 00:01:44,740
padded values in each sequence are ignored.

50
00:01:45,200 --> 00:01:47,220
The dropout layer helps prevent overfitting.

51
00:01:47,930 --> 00:01:51,750
The two normalization layers is to help accelerate our convergence.

52
00:01:53,390 --> 00:01:55,690
Our second model is an LSTM-based model.

53
00:01:56,330 --> 00:02:00,190
The two-layer LSTM helps our model to learn hierarchical representation.

54
00:02:00,190 --> 00:02:05,590
Our third model is based on the second model,

55
00:02:06,030 --> 00:02:07,630
but with attention layers added.

56
00:02:08,330 --> 00:02:10,930
Our attention is implemented with the following formula,

57
00:02:11,810 --> 00:02:14,250
and it's an additive-based attention.

58
00:02:16,660 --> 00:02:18,640
Our first model is a specialist model,

59
00:02:19,220 --> 00:02:23,740
where we have a normal model which helps predicting the festival pitch types,

60
00:02:24,400 --> 00:02:27,400
and a specialized model helps predicting off-speed pitch types.

61
00:02:28,170 --> 00:02:28,590
And finally,

62
00:02:28,650 --> 00:02:32,850
we have a voting system that takes in the input from those two models

63
00:02:32,850 --> 00:02:34,270
and outputs our final prediction.

64
00:02:34,270 --> 00:02:38,310
Experiment to address the limited data size,

65
00:02:38,530 --> 00:02:40,830
we adopted a slicing technique inspired by

66
00:02:41,650 --> 00:02:42,390
reference paper.

67
00:02:42,630 --> 00:02:48,630
We used beta-based and data-based as criteria for slicing to increase the data size.

68
00:02:50,630 --> 00:02:51,210
For example,

69
00:02:51,470 --> 00:02:57,590
if the minima subsequence length is two and the max length is three,

70
00:02:57,970 --> 00:02:58,310
and we

71
00:02:58,310 --> 00:03:00,020
get this,

72
00:03:00,100 --> 00:03:00,700
this,

73
00:03:01,040 --> 00:03:01,480
this,

74
00:03:01,900 --> 00:03:02,320
this,

75
00:03:02,780 --> 00:03:08,680
and this subsequence and add this subsequence to our data set.

76
00:03:11,340 --> 00:03:12,580
For our data splitting part,

77
00:03:12,680 --> 00:03:14,400
our data is split with the following ratio.

78
00:03:15,140 --> 00:03:19,540
And note that our training plus validation set and our testing set are split without

79
00:03:19,540 --> 00:03:20,760
breaking the temporal order.

80
00:03:21,260 --> 00:03:22,320
This is because otherwise,

81
00:03:22,540 --> 00:03:24,140
subsequences in the training set

82
00:03:24,140 --> 00:03:26,540
might contain future information of our testing set.

83
00:03:27,310 --> 00:03:27,810
And finally,

84
00:03:27,910 --> 00:03:29,270
our subsequences in each of

85
00:03:29,270 --> 00:03:31,010
Closed sets are randomly shuffled.

86
00:03:32,940 --> 00:03:36,620
Here are the results of each prediction model for 3 pitchers.

87
00:03:38,240 --> 00:03:40,300
Starting with Shota Imunaga,

88
00:03:40,380 --> 00:03:41,180
a starting pitcher.

89
00:03:41,660 --> 00:03:43,580
We used BayerServe as a baseline,

90
00:03:44,120 --> 00:03:45,300
and used SimpleRNN,

91
00:03:45,660 --> 00:03:46,340
LSTM,

92
00:03:46,600 --> 00:03:48,300
and LSTM with Attention as models.

93
00:03:48,840 --> 00:03:53,920
The SimpleRNN demonstrated significant improvements that reduced loss and increased accuracy.

94
00:03:55,730 --> 00:03:56,170
Next,

95
00:03:56,650 --> 00:03:57,530
for Hunter Green,

96
00:03:57,870 --> 00:03:59,010
a starting pitcher too.

97
00:03:59,010 --> 00:04:03,590
We observed that their three advanced models did not significantly improve accuracy.

98
00:04:04,170 --> 00:04:04,550
However,

99
00:04:04,990 --> 00:04:06,030
among these models,

100
00:04:06,430 --> 00:04:08,410
LSTM with ATTENTION performed the best.

101
00:04:10,030 --> 00:04:10,650
Finally,

102
00:04:11,050 --> 00:04:11,930
for Mason Miller,

103
00:04:12,330 --> 00:04:13,290
a relief pitcher,

104
00:04:13,810 --> 00:04:17,149
he achieved the highest prediction accuracy among the three pitchers.

105
00:04:17,649 --> 00:04:20,890
Simple RN and LSTM with ATTENTION stood out in particular,

106
00:04:21,430 --> 00:04:23,050
performed exceptionally well.

107
00:04:24,600 --> 00:04:24,940
Okay,

108
00:04:25,040 --> 00:04:26,300
so now let's go through some discussion.

109
00:04:26,620 --> 00:04:30,560
The first thing is that the accuracy in lotteries are not enough to evaluate models.

110
00:04:30,560 --> 00:04:31,260
At first,

111
00:04:31,340 --> 00:04:32,800
we really rely on it to train models,

112
00:04:32,900 --> 00:04:34,720
but after generating some confusion metrics,

113
00:04:34,840 --> 00:04:37,480
we find out that we uncover a really significant issue,

114
00:04:37,600 --> 00:04:38,760
is that the models just frequently

115
00:04:38,760 --> 00:04:40,600
predict fastballs in all the cases,

116
00:04:40,700 --> 00:04:42,680
even the ground truth is other pitch types.

117
00:04:42,900 --> 00:04:45,960
And we think that the behavior happens because the fastball are the main pitch type for most

118
00:04:45,960 --> 00:04:46,340
of the pitcher,

119
00:04:46,520 --> 00:04:50,140
and we can prove it by these distribution figures of our three pitchers

120
00:04:50,140 --> 00:04:50,520
we used.

121
00:04:50,920 --> 00:04:51,700
To address this issue,

122
00:04:51,800 --> 00:04:52,700
after reading some papers,

123
00:04:52,880 --> 00:04:54,660
we add a specialist model which focuses

124
00:04:54,660 --> 00:04:55,880
on less frequent pitch types,

125
00:04:56,080 --> 00:04:58,600
while the original model still exists to handle general cases.

126
00:04:58,600 --> 00:05:01,580
Then using a voting method to decide which model output to choose,

127
00:05:01,920 --> 00:05:02,400
while the overall

128
00:05:02,400 --> 00:05:04,240
accuracy doesn't really improve a lot,

129
00:05:04,400 --> 00:05:07,060
we can still find that not only reduce the probability

130
00:05:07,060 --> 00:05:10,320
of wrong predicting the fastball in wrong time,

131
00:05:10,460 --> 00:05:12,680
but also has a better ability to predict

132
00:05:12,680 --> 00:05:15,360
less frequent pitch types such as splitter or curveballs.

133
00:05:15,800 --> 00:05:20,140
The second point is that the LSTM or adding the attention layer do not really outperform

134
00:05:20,140 --> 00:05:22,420
on the short sequence by results as below.

135
00:05:22,840 --> 00:05:26,460
And actually our sub-sequence range from the two to six pitches,

136
00:05:26,700 --> 00:05:27,620
which are relatively short,

137
00:05:27,620 --> 00:05:30,120
so that the long-term memory advantage of LSTM

138
00:05:30,120 --> 00:05:32,060
do not really fully utilize.

139
00:05:32,520 --> 00:05:35,200
And it may even cause some potential bias accumulation,

140
00:05:35,400 --> 00:05:38,500
which means that it over-emphasize the patterns

141
00:05:38,500 --> 00:05:41,120
or over-focus on high-frequency inputs like fastball

142
00:05:41,120 --> 00:05:44,020
and neglect lots of other pitch types.

143
00:05:45,770 --> 00:05:46,030
Okay,

144
00:05:46,230 --> 00:05:47,250
let's talk about conclusion.

145
00:05:47,670 --> 00:05:49,610
The first thing is that our model outperformed

146
00:05:49,610 --> 00:05:50,330
Bayes' baseline,

147
00:05:50,430 --> 00:05:51,050
which is really good,

148
00:05:51,110 --> 00:05:51,330
you know?

149
00:05:51,450 --> 00:05:53,770
And the second thing is that our model performed better

150
00:05:53,770 --> 00:05:56,070
when pitchers have fewer pitch types

151
00:05:56,070 --> 00:05:57,830
like Miller or Green.

152
00:05:57,830 --> 00:06:02,130
but our model still remains solid and not bad

153
00:06:02,130 --> 00:06:04,950
when the pitcher have more pitch types like Shota Imanaga.

154
00:06:05,490 --> 00:06:08,130
And our model really have some practical value.

155
00:06:08,330 --> 00:06:10,390
It's that they provide a powerful tool

156
00:06:10,390 --> 00:06:12,970
to analyze the pitching tendencies and some strategies.

157
00:06:13,390 --> 00:06:14,270
For pitcher and coaches,

158
00:06:14,390 --> 00:06:15,830
they can use it to check

159
00:06:15,830 --> 00:06:18,610
that if there are some maybe subtle differences

160
00:06:18,610 --> 00:06:21,490
between maybe release points or predictable sequences

161
00:06:21,950 --> 00:06:23,930
might allow batter to guess pitches.

162
00:06:24,370 --> 00:06:26,390
And addressing this issue can make pitcher

163
00:06:26,390 --> 00:06:27,730
have better performance.

164
00:06:28,190 --> 00:06:29,010
And for catchers,

165
00:06:29,190 --> 00:06:31,770
they can use this model to check that if there are

166
00:06:31,770 --> 00:06:35,050
really some habit patterns when they are calling set strategies.

167
00:06:35,330 --> 00:06:37,070
And this can help them to have a

168
00:06:37,070 --> 00:06:38,390
better performance.

169
00:06:38,770 --> 00:06:39,290
And that's it.

170
00:06:39,450 --> 00:06:40,250
Thank you for listening.

171
00:06:40,470 --> 00:06:41,470
And we're team 23.

