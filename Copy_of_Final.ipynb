{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g2nBlgOh_dJD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# from tensorflow.keras.optimizers.schedules import ExponentialDecay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('shota_imanaga_2024.csv')\n",
        "data = data.iloc[::-1].reset_index(drop=True)\n",
        "\n",
        "# Encode the target column (Pitch type)\n",
        "label_encoder = LabelEncoder()\n",
        "data['Pitch type'] = label_encoder.fit_transform(data['Pitch type'])\n",
        "\n",
        "# One-hot encode the 'Description' column\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "description_encoded = one_hot_encoder.fit_transform(data[['Description']]).toarray()\n",
        "\n",
        "# Add one-hot encoded columns back to the DataFrame\n",
        "description_columns = [f\"Description_{i}\" for i in range(description_encoded.shape[1])]\n",
        "description_df = pd.DataFrame(description_encoded, columns=description_columns)\n",
        "\n",
        "# Concatenate one-hot encoded data with the original data\n",
        "data = pd.concat([data.reset_index(drop=True), description_df.reset_index(drop=True)], axis=1)\n",
        "data = data.drop(columns=['Description'])  # Drop the original 'Description' column\n"
      ],
      "metadata": {
        "id": "g85QthLcD4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "026bd778-e970-49b6-ca0b-43b3c6d624b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'shota_imanaga_2024.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3e4a75d39ef5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shota_imanaga_2024.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encode the target column (Pitch type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shota_imanaga_2024.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "PVmiGHP2ENa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, sequence_length, feature_columns, target_column):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for _, group in df.groupby('Date'):  # Group by game\n",
        "        features = group[feature_columns].values\n",
        "        targets_group = group[target_column].values\n",
        "\n",
        "        # Create sequences within each group\n",
        "        for i in range(len(features) - sequence_length):\n",
        "            sequences.append(features[i:i + sequence_length])  # Sequence of pitches\n",
        "            targets.append(targets_group[i + sequence_length])  # Next pitch type\n",
        "\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_columns = ['Batter ID', 'isStrike', 'Zone', 'Strike Detail']\n",
        "target_column = 'Pitch type'\n",
        "sequence_length = 5\n",
        "\n",
        "# Generate sequences\n",
        "X, y = create_sequences(data, sequence_length, feature_columns, target_column)\n",
        "\n",
        "# Pad sequences to a uniform length\n",
        "X = pad_sequences(X, maxlen=sequence_length, dtype='float32', padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "cuBhjA15Epgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# print(data.groupby('Date').size())"
      ],
      "metadata": {
        "id": "NfLxI8h_E11e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define split ratios\n",
        "train_ratio = 0.68\n",
        "val_ratio = 0.12\n",
        "test_ratio = 0.2\n",
        "\n",
        "# First, split into training and temp (validation + testing)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_ratio), random_state=42)\n",
        "\n",
        "# Then, split temp into validation and testing\n",
        "val_test_ratio = test_ratio / (test_ratio + val_ratio)  # Adjust split ratio for remaining data\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_test_ratio, random_state=42)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "X2NijUf5Knrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERSION: simple RNN\n",
        "# # Number of unique pitch types (replace with actual number from your data)\n",
        "# num_classes = len(set(y))  # Assuming `y` is label-encoded\n",
        "\n",
        "# # Build the model\n",
        "# model = Sequential([\n",
        "#     Input(shape=(X_train.shape[1], X_train.shape[2])),  # Input: (sequence_length, num_features)\n",
        "#     SimpleRNN(64, return_sequences=False),  # RNN with 64 units\n",
        "#     Dense(32, activation='relu'),           # Fully connected layer\n",
        "#     Dense(num_classes, activation='softmax')  # Output layer for classification\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='sparse_categorical_crossentropy',  # Use sparse_categorical_crossentropy for label-encoded targets\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# # VERSION: GRU\n",
        "# num_classes = len(set(y))  # Assuming `y` is label-encoded\n",
        "# # Hyperparameters\n",
        "# embedding_dim = 16\n",
        "# gru_units = 128\n",
        "# dense_units = 64\n",
        "# dropout_rate = 0.3\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# # Model architecture\n",
        "# model = Sequential([\n",
        "#     # # Embedding layer for Batter ID if needed (only for categorical IDs)\n",
        "#     # Embedding(input_dim=1000,  # Adjust based on unique values in Batter ID\n",
        "#     #           output_dim=embedding_dim,\n",
        "#     #           input_length=X_train.shape[1]),  # Only if Batter ID is a primary feature\n",
        "\n",
        "#     # GRU layer\n",
        "#     GRU(gru_units, return_sequences=False),\n",
        "\n",
        "#     # Batch Normalization\n",
        "#     BatchNormalization(),\n",
        "\n",
        "#     # Fully connected dense layer\n",
        "#     Dense(dense_units, activation='relu'),\n",
        "\n",
        "#     # Dropout for regularization\n",
        "#     Dropout(dropout_rate),\n",
        "\n",
        "#     # Output layer for classification\n",
        "#     Dense(num_classes, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(\n",
        "#     optimizer=Adam(learning_rate=learning_rate),\n",
        "#     loss='sparse_categorical_crossentropy',  # Use for integer-encoded targets\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# VERSION: LSTM\n",
        "# Number of unique pitch types (target classes)\n",
        "num_classes = len(set(y))  # Assuming `y` is label-encoded\n",
        "\n",
        "# Hyperparameters\n",
        "lstm_units = 128\n",
        "dense_units_1 = 64\n",
        "dense_units_2 = 32\n",
        "dropout_rate = 0.4\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    # First LSTM layer\n",
        "    LSTM(lstm_units, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), recurrent_dropout=0.2),\n",
        "\n",
        "    # Batch Normalization\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Second LSTM layer (stacked)\n",
        "    LSTM(lstm_units // 2, return_sequences=False, recurrent_dropout=0.2),\n",
        "\n",
        "    # Batch Normalization\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Fully connected dense layer 1\n",
        "    Dense(dense_units_1, activation='relu'),\n",
        "\n",
        "    # Dropout for regularization\n",
        "    Dropout(dropout_rate),\n",
        "\n",
        "    # Fully connected dense layer 2\n",
        "    Dense(dense_units_2, activation='relu'),\n",
        "\n",
        "    # Dropout for regularization\n",
        "    Dropout(dropout_rate),\n",
        "\n",
        "    # Output layer for classification\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    # optimizer=Adam(learning_rate=ExponentialDecay(initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.96)),\n",
        "    optimizer=Adam(learning_rate=learning_rate),\n",
        "    loss='sparse_categorical_crossentropy',  # Use for integer-encoded targets\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "wjTU-RFKLDqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = model.fit(\n",
        "#     X_train, y_train,          # Training data\n",
        "#     validation_data=(X_val, y_val),  # Validation data\n",
        "#     epochs=20,                 # Number of epochs\n",
        "#     batch_size=32,             # Batch size\n",
        "#     verbose=1                  # Verbosity level\n",
        "# )\n",
        "\n",
        "\n",
        "# # VERSION: GRU\n",
        "# # Callbacks for better training\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(\n",
        "#     X_train, y_train,\n",
        "#     validation_data=(X_val, y_val),\n",
        "#     epochs=30,                   # Increase if no overfitting\n",
        "#     batch_size=32,               # Adjust for GPU/CPU memory limits\n",
        "#     callbacks=[reduce_lr, early_stopping],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "\n",
        "# VERSION: LSTM\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "464S1eqgLJLd",
        "outputId": "e54ee73b-5687-4a89-d166-828c7360c93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - accuracy: 0.0431 - loss: 2.1805 - val_accuracy: 0.0000e+00 - val_loss: 2.1309 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.0924 - loss: 2.1125 - val_accuracy: 0.0000e+00 - val_loss: 2.0836 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1364 - loss: 2.0751 - val_accuracy: 0.2662 - val_loss: 2.0458 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.2200 - loss: 2.0409 - val_accuracy: 0.2662 - val_loss: 2.0217 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.2972 - loss: 2.0136 - val_accuracy: 0.5290 - val_loss: 1.9961 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3684 - loss: 1.9715 - val_accuracy: 0.5290 - val_loss: 1.9645 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.3658 - loss: 1.9231 - val_accuracy: 0.5290 - val_loss: 1.9162 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.3934 - loss: 1.8337 - val_accuracy: 0.5290 - val_loss: 1.8381 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4028 - loss: 1.7385 - val_accuracy: 0.5290 - val_loss: 1.7155 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4207 - loss: 1.6094 - val_accuracy: 0.5290 - val_loss: 1.6091 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4531 - loss: 1.5223 - val_accuracy: 0.5290 - val_loss: 1.5515 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4383 - loss: 1.5438 - val_accuracy: 0.5290 - val_loss: 1.5559 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4729 - loss: 1.4750 - val_accuracy: 0.5290 - val_loss: 1.6844 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4507 - loss: 1.4891\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4509 - loss: 1.4890 - val_accuracy: 0.5290 - val_loss: 1.6945 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.4554 - loss: 1.4426 - val_accuracy: 0.5290 - val_loss: 1.6935 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.4648 - loss: 1.4322 - val_accuracy: 0.5290 - val_loss: 1.6645 - learning_rate: 5.0000e-05\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=1)\n",
        "print(f\"training Loss: {train_loss}\")\n",
        "print(f\"training Accuracy: {train_accuracy}\")\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
        "print(f\"valing Loss: {val_loss}\")\n",
        "print(f\"valing Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZT7p8yoLkI2",
        "outputId": "eb2850eb-4d14-47f7-924d-93f62561b54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4910 - loss: 1.5671\n",
            "training Loss: 1.5489451885223389\n",
            "training Accuracy: 0.5042117834091187\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5247 - loss: 1.5483 \n",
            "valing Loss: 1.551474690437317\n",
            "valing Accuracy: 0.5290102362632751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Testing Loss: {test_loss}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boGBpY-zLRwm",
        "outputId": "6f72cc26-7f59-4817-c244-5b8bb55cd872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4994 - loss: 1.5370\n",
            "Testing Loss: 1.5520493984222412\n",
            "Testing Accuracy: 0.4938775599002838\n"
          ]
        }
      ]
    }
  ]
}